DOCKER_BUILDER_CONTAINER=minute
APP_NAME=minute
ENVIRONMENT=local
LOG_LEVEL=DEBUG
APP_URL=http://localhost:3000
BACKEND_HOST=http://localhost:8080

POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=minute_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=insecure

TRANSCRIPTION_QUEUE_NAME=minute-transcription-queue
TRANSCRIPTION_DEADLETTER_QUEUE_NAME=minute-transcription-queue-deadletter
LLM_QUEUE_NAME=minute-llm-queue
LLM_DEADLETTER_QUEUE_NAME=minute-llm-queue-deadletter

REPO=minute
AUTH_API_URL=http://localhost:8080
DISABLE_AUTH_SIGNATURE_VERIFICATION=true

USE_LOCALSTACK=true
LOCALSTACK_URL=http://localhost:4566
STORAGE_SERVICE_NAME=local
LOCAL_STORAGE_PATH=./.data
QUEUE_SERVICE_NAME=sqs

TRANSCRIPTION_SERVICES=["whisply_local"]

WHISPLY_DEVICE=mps
WHISPLY_MODEL=small
WHISPLY_LANGUAGE=en
WHISPLY_ENABLE_DIARIZATION=true
WHISPLY_HF_TOKEN=your_huggingface_token_here
WHISPLY_TIMEOUT=3600

HF_HOME=/Users/patkuc/.cache/huggingface
TRANSFORMERS_CACHE=/Users/patkuc/.cache/huggingface/hub

OPENAI_DIRECT_API_KEY=

# === LLM config ===
# Option 1: Use Ollama (local, recommended for development)
# Recommended quantized models (8B for structured outputs):
#   - llama3.1:8b-instruct-q4_K_M
# Download: ollama pull llama3.1:8b-instruct-q4_K_M
FAST_LLM_PROVIDER=ollama
FAST_LLM_MODEL_NAME=llama3.1:8b-instruct-q4_K_M
BEST_LLM_PROVIDER=ollama
BEST_LLM_MODEL_NAME=llama3.1:8b-instruct-q4_K_M

OLLAMA_BASE_URL=http://localhost:11434/v1

AZURE_SPEECH_KEY=not_needed_for_local
AZURE_SPEECH_REGION=not_needed_for_local
