# -*- coding: utf-8 -*-
"""transcription eval

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uPUhvUtrIF4w6VjQJO9CqmSztiLByulb
"""

!apt-get -y update >/dev/null
!apt-get -y install ffmpeg >/dev/null

!pip -q install \
  datasets==2.* \
  evaluate==0.4.* \
  jiwer==3.* \
  soundfile==0.12.* \
  librosa==0.10.* \
  azure-cognitiveservices-speech==1.* \
  openai-whisper==20231117

import os

try:
    from google.colab import userdata
    AZURE_SPEECH_KEY = userdata.get("AZURE_SPEECH_KEY")
    AZURE_SPEECH_REGION = userdata.get("AZURE_SPEECH_REGION")
except Exception:
    AZURE_SPEECH_KEY = None
    AZURE_SPEECH_REGION = None

AZURE_SPEECH_KEY = AZURE_SPEECH_KEY or os.environ.get("AZURE_SPEECH_KEY")
AZURE_SPEECH_REGION = AZURE_SPEECH_REGION or os.environ.get("AZURE_SPEECH_REGION")

print("Azure key present:", bool(AZURE_SPEECH_KEY))
print("Azure region present:", bool(AZURE_SPEECH_REGION))

"""
DATASET SHAPE CONTRACT FOR THIS BENCHMARK PIPELINE

This pipeline assumes a Hugging Face–style dataset where each row (example)
behaves like a dictionary with the following REQUIRED fields.

This is intentionally simple. If you can adapt your dataset to this shape,
the rest of the benchmark code will work unchanged.

REQUIRED PER-ROW SHAPE (treat this as true):

example = {
    "audio": {
        "array": <1D numpy array of audio samples>,   # mono audio
        "sampling_rate": <int>,                        # e.g. 16000, 22050, 44100
    },
    "text": "<ground truth transcription as string>"
}

Nothing else is required by the runner.
"""

import os
import librosa
import soundfile as sf
from datasets import load_dataset

# ---------------------------------------------------------------------
# Dataset loading (example: LibriSpeech via Hugging Face)
# Replace this block if you use a different dataset source,
# but keep the per-row shape described above.
# ---------------------------------------------------------------------

DATASET_NAME = "librispeech_asr"
DATASET_CONFIG = "clean"
DATASET_SPLIT = "test"

WORKDIR = "/content/stt_benchmark_poc"
AUDIO_DIR = os.path.join(WORKDIR, "audio")
os.makedirs(AUDIO_DIR, exist_ok=True)

ds = load_dataset(DATASET_NAME, DATASET_CONFIG, split=DATASET_SPLIT)

print("Loaded dataset:", DATASET_NAME, DATASET_CONFIG, DATASET_SPLIT)
print("Number of rows:", len(ds))

# ---------------------------------------------------------------------
# Helper: write audio to disk as mono 16 kHz PCM WAV
#
# Assumptions:
# - example["audio"]["array"] is a 1D array (mono)
#   If your dataset is stereo (shape [N, 2]), convert to mono first.
# - example["audio"]["sampling_rate"] exists
# ---------------------------------------------------------------------

def to_wav_16k_mono(example, idx: int) -> str:
    audio = example["audio"]

    y = audio["array"]
    sr = audio["sampling_rate"]

    # If stereo, convert to mono by averaging channels
    if getattr(y, "ndim", 1) == 2:
        y = y.mean(axis=1)

    # Resample to 16 kHz if needed
    if sr != 16000:
        y = librosa.resample(y, orig_sr=sr, target_sr=16000)

    path = os.path.join(AUDIO_DIR, f"sample_{idx:06d}.wav")
    sf.write(path, y, 16000, subtype="PCM_16")
    return path

# ---------------------------------------------------------------------
# Helper: compute duration of an audio file in seconds
#
# Assumptions:
# - wav_path points to a readable audio file
# - audio is mono or stereo (librosa will handle both)
# ---------------------------------------------------------------------

def audio_duration_seconds(wav_path: str) -> float:
    y, sr = librosa.load(wav_path, sr=None, mono=True)
    return float(len(y) / sr)

# ---------------------------------------------------------------------
# Contract check (run this once after loading a new dataset)
#
# This fails fast if the dataset does not match the expected shape.
# Keep this when swapping datasets so you know exactly what to adapt.
# ---------------------------------------------------------------------

ex0 = ds[0]

assert "audio" in ex0, "Dataset row must contain 'audio'"
assert "text" in ex0, "Dataset row must contain 'text'"

assert "array" in ex0["audio"], "audio must contain 'array'"
assert "sampling_rate" in ex0["audio"], "audio must contain 'sampling_rate'"
assert isinstance(ex0["text"], str), "'text' must be a string transcript"

audio_array = ex0["audio"]["array"]
sampling_rate = ex0["audio"]["sampling_rate"]

print("\nDataset contract check passed.")
print("Example text type:", type(ex0["text"]))
print("Audio array ndim:", getattr(audio_array, "ndim", None))
print("Audio array shape:", getattr(audio_array, "shape", None))
print("Sampling rate:", sampling_rate)

"""
HOW TO ADAPT A DIFFERENT DATASET

If your dataset does not match this shape, adapt it so that:
- You expose an 'audio' dict with 'array' and 'sampling_rate'
- You expose a 'text' string with the ground truth transcript

Typical fixes:
- Rename transcript column:
    ds = ds.rename_column("transcript", "text")

- Convert stereo to mono:
    y = y.mean(axis=1)

- Wrap raw audio into the expected dict:
    example["audio"] = {"array": y, "sampling_rate": sr}

Once this contract is satisfied, the benchmark runner will work unchanged.
"""

# Replace your normalisation + metrics cell with this concise version

import re
import numpy as np
from jiwer import wer

_unicode_map = str.maketrans({
    "’": "'", "‘": "'", "“": '"', "”": '"',
    "–": " ", "—": " ", "…": " ",
})

_non_word = re.compile(r"[^\w\s']+", re.UNICODE)
_ws = re.compile(r"\s+", re.UNICODE)

def normalise_text(s: str) -> str:
    s = (s or "").strip().lower().translate(_unicode_map)
    s = _non_word.sub(" ", s)
    return _ws.sub(" ", s).strip()

def compute_wer_pct(refs, hyps) -> float:
    return 100.0 * wer([normalise_text(r) for r in refs], [normalise_text(h) for h in hyps])

class TimingAccumulator:
    def __init__(self):
        self.process_sec = 0.0
        self.audio_sec = 0.0
    def add(self, audio_sec, process_sec):
        self.audio_sec += float(audio_sec); self.process_sec += float(process_sec)
    @property
    def rtf(self):
        return self.process_sec / self.audio_sec if self.audio_sec else float("nan")

# Replace your evaluation runner with this concise debug-enabled runner
# Uses jiwer.wer for WER, difflib for a lightweight token diff summary, and writes JSON.

import json
from difflib import SequenceMatcher
from jiwer import wer

def _token_ops(a: str, b: str) -> dict:
    a = (a or "").split()
    b = (b or "").split()
    sm = SequenceMatcher(a=a, b=b)
    ops = {"equal": 0, "replace": 0, "delete": 0, "insert": 0}
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal": ops["equal"] += (i2 - i1)
        elif tag == "replace": ops["replace"] += max(i2 - i1, j2 - j1)
        elif tag == "delete": ops["delete"] += (i2 - i1)
        elif tag == "insert": ops["insert"] += (j2 - j1)
    return ops

def run_engine(adapter, indices, label, *, dataset, wav_write_fn, duration_fn,
               json_out_path="/content/stt_debug.json", print_examples=3):
    rows = []
    timing = TimingAccumulator()

    print(f"\n=== {label} ({len(indices)} sample(s)) ===")
    print("Debug JSON:", json_out_path)

    for n, idx in enumerate(indices, start=1):
        ex = dataset[int(idx)]
        wav_path = wav_write_fn(ex, int(idx))
        ref_raw = ex["text"]
        aud_sec = float(duration_fn(wav_path))

        hyp_raw, proc_sec, dbg = adapter.transcribe_with_debug(wav_path)

        proc_sec = float(proc_sec)
        timing.add(aud_sec, proc_sec)

        ref_n = normalise_text(ref_raw)
        hyp_n = normalise_text(hyp_raw)

        per_wer = 100.0 * wer([ref_n], [hyp_n])
        ops = _token_ops(ref_n, hyp_n)

        row = {
            "engine": label,
            "dataset_index": int(idx),
            "wav_path": wav_path,
            "audio_sec": aud_sec,
            "process_sec": proc_sec,
            "rtf": (proc_sec / aud_sec) if aud_sec else None,
            "wer_pct": float(per_wer),
            "diff_ops": ops,
            "ref_raw": ref_raw,
            "hyp_raw": hyp_raw,
            "ref_norm": ref_n,
            "hyp_norm": hyp_n,
            "engine_debug": dbg,
        }
        rows.append(row)

        print(f"[{label}] {n}/{len(indices)} idx={idx} aud_sec={aud_sec:.2f} proc_sec={proc_sec:.2f} WER={per_wer:.2f}")

        if n <= print_examples:
            print("  ref_norm:", ref_n[:200])
            print("  hyp_norm:", hyp_n[:200])
            print("  diff_ops:", ops)

    overall_wer = compute_wer_pct([r["ref_raw"] for r in rows], [r["hyp_raw"] for r in rows])
    per_wers = [r["wer_pct"] for r in rows]

    summary = {
        "engine": label,
        "num_samples": len(indices),
        "overall_wer_pct": float(overall_wer),
        "rtf": float(timing.rtf),
        "process_sec": float(timing.process_sec),
        "audio_sec": float(timing.audio_sec),
        "per_sample_wer_min": float(np.min(per_wers)) if per_wers else None,
        "per_sample_wer_max": float(np.max(per_wers)) if per_wers else None,
        "per_sample_wer_mean": float(np.mean(per_wers)) if per_wers else None,
    }

    out = {"summary": summary, "samples": rows}
    with open(json_out_path, "w", encoding="utf-8") as f:
        json.dump(out, f, indent=2, ensure_ascii=False)

    print("\nSummary:")
    print(json.dumps(summary, indent=2))
    return out

import time
import threading
import azure.cognitiveservices.speech as speechsdk

class AzureSTTAdapter:
    """
    Azure STT adapter that supports long audio by using continuous recognition.

    Why this exists
    - recognize_once_async() returns after a single utterance (silence based, with a cap).
    - start_continuous_recognition_async() is the correct API for multi-utterance / long audio. :contentReference[oaicite:2]{index=2}

    Methods
    - transcribe(wav_path) -> (full_text, process_seconds)
    - transcribe_with_debug(wav_path) -> (full_text, process_seconds, debug_dict)
    """

    def __init__(self, speech_key: str, speech_region: str, language: str = "en-US"):
        if not speech_key or not speech_region:
            raise ValueError("Azure speech key and region are required")
        self.speech_key = speech_key
        self.speech_region = speech_region
        self.language = language

    def _make_recogniser(self, wav_path: str):
        speech_config = speechsdk.SpeechConfig(subscription=self.speech_key, region=self.speech_region)
        speech_config.speech_recognition_language = self.language

        audio_config = speechsdk.audio.AudioConfig(filename=wav_path)
        recogniser = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        return recogniser

    def transcribe(self, wav_path: str):
        text, proc_sec, _ = self.transcribe_with_debug(wav_path)
        return text, proc_sec

    def transcribe_with_debug(self, wav_path: str):
        recogniser = self._make_recogniser(wav_path)

        parts = []
        debug = {
            "mode": "continuous",
            "recognized_segments": [],
            "canceled": None,
            "session_stopped": False,
        }

        done = threading.Event()

        def on_recognized(evt):
            res = evt.result
            if res.reason == speechsdk.ResultReason.RecognizedSpeech and (res.text or "").strip():
                parts.append(res.text.strip())
                debug["recognized_segments"].append({
                    "text": res.text,
                    "offset": getattr(res, "offset", None),
                    "duration": getattr(res, "duration", None),
                })

        def on_canceled(evt):
            cd = speechsdk.CancellationDetails.from_result(evt.result)
            debug["canceled"] = {
                "reason": str(cd.reason),
                "error_code": str(cd.error_code),
                "error_details": cd.error_details,
            }
            done.set()

        def on_session_stopped(evt):
            debug["session_stopped"] = True
            done.set()

        recogniser.recognized.connect(on_recognized)
        recogniser.canceled.connect(on_canceled)
        recogniser.session_stopped.connect(on_session_stopped)

        t0 = time.time()
        recogniser.start_continuous_recognition_async().get()
        done.wait()  # wait until file finished or cancelled
        recogniser.stop_continuous_recognition_async().get()
        t1 = time.time()

        full_text = " ".join(parts).strip()
        debug["final_text_len_chars"] = len(full_text)
        debug["final_text_len_words"] = len(full_text.split())

        return full_text, (t1 - t0), debug

import time
import whisper

class WhisperAdapter:
    """
    Whisper adapter.

    Provides:
    - transcribe(wav_path) -> (text, process_seconds)
    - transcribe_with_debug(wav_path) -> (text, process_seconds, debug_dict)

    Debug info is minimal because Whisper does not expose rich internal signals.
    """

    def __init__(self, model_name: str = "base", language: str = "en"):
        self.model_name = model_name
        self.language = language
        self.model = whisper.load_model(model_name)

    def transcribe(self, wav_path: str):
        t0 = time.time()
        out = self.model.transcribe(
            wav_path,
            language=self.language,
            fp16=False,
        )
        t1 = time.time()

        text = out.get("text", "") or ""
        return text, (t1 - t0)

    def transcribe_with_debug(self, wav_path: str):
        t0 = time.time()
        out = self.model.transcribe(
            wav_path,
            language=self.language,
            fp16=False,
        )
        t1 = time.time()

        debug = {
            "model": self.model_name,
            "language": self.language,
            "segments": len(out.get("segments", [])),
        }

        text = out.get("text", "") or ""
        return text, (t1 - t0), debug

import numpy as np

# Hyperparameter
NUM_LONGEST_SAMPLES = 10  # change this freely

print(f"Selecting the {NUM_LONGEST_SAMPLES} longest samples from the dataset")

durations = []

for idx, ex in enumerate(ds):
    audio = ex["audio"]
    dur_sec = len(audio["array"]) / audio["sampling_rate"]
    durations.append((idx, dur_sec))

# Sort by duration descending
durations_sorted = sorted(durations, key=lambda x: x[1], reverse=True)

selected = durations_sorted[:NUM_LONGEST_SAMPLES]

indices = [idx for idx, _ in selected]
lengths = [dur for _, dur in selected]

print("\nSelected sample indices and durations:")
for idx, dur in selected:
    print(f"  idx={idx} duration_sec={dur:.2f}")

total_sec = float(np.sum(lengths))
min_sec = float(np.min(lengths))
max_sec = float(np.max(lengths))
mean_sec = float(np.mean(lengths))

print("\nSummary statistics for selected samples:")
print(f"  count: {len(lengths)}")
print(f"  total_sec: {total_sec:.2f}")
print(f"  min_sec: {min_sec:.2f}")
print(f"  max_sec: {max_sec:.2f}")
print(f"  mean_sec: {mean_sec:.2f}")

# These indices can now be used for both Azure and Whisper
SELECTED_INDICES = indices

import json

azure_indices = list(SELECTED_INDICES)
whisper_indices = list(SELECTED_INDICES)

print(f"Azure will run {len(azure_indices)} selected cases: {azure_indices}")
print(f"Whisper will run {len(whisper_indices)} selected cases: {whisper_indices}")

azure_adapter = AzureSTTAdapter(
    speech_key=AZURE_SPEECH_KEY,
    speech_region=AZURE_SPEECH_REGION,
    language="en-US",  # try "en-GB" as a quick sanity check
)

whisper_adapter = WhisperAdapter(
    model_name="large",
    language="en",
)

azure_out = run_engine(
    adapter=azure_adapter,
    indices=azure_indices,
    label=f"Azure Speech-to-Text (selected {len(azure_indices)})",
    dataset=ds,
    wav_write_fn=to_wav_16k_mono,
    duration_fn=audio_duration_seconds,
    json_out_path="/content/azure_debug.json",
    print_examples=len(azure_indices),
)

whisper_out = run_engine(
    adapter=whisper_adapter,
    indices=whisper_indices,
    label=f"Whisper (selected {len(whisper_indices)})",
    dataset=ds,
    wav_write_fn=to_wav_16k_mono,
    duration_fn=audio_duration_seconds,
    json_out_path="/content/whisper_debug.json",
    print_examples=len(whisper_indices),
)

print("\n=== Combined summary ===")
print(json.dumps([azure_out["summary"], whisper_out["summary"]], indent=2))

"""
SNIPPET 1: Select the N longest samples and build a DSPy "transcript-fix" dataset

What this is for
- Build a supervised dataset for "transcript repair":
  input  = Whisper transcript (noisy)
  label  = ground truth transcript (gold)
- We pick the N longest samples to get more varied and challenging content.
- The output is JSONL so you can reuse it later without re-running Whisper.

Requirements already defined in previous cells
- ds: HF dataset, each row has:
    - example["audio"]["array"]
    - example["audio"]["sampling_rate"]
    - example["text"]
- to_wav_16k_mono(example, idx) -> wav_path
- audio_duration_seconds(wav_path) -> seconds
- normalise_text(text) -> normalised string
- WhisperAdapter class (or whisper_adapter already initialised)

Outputs
- SELECTED_INDICES_LONGEST: indices of selected rows
- /content/dspy_transcript_fix_dataset.jsonl: JSONL dataset
"""

import os
import json
import numpy as np
from pathlib import Path

# Hyperparameters
NUM_LONGEST_SAMPLES = 200
OUT_PATH = "/content/dspy_transcript_fix_dataset.jsonl"
REUSE_IF_EXISTS = True

# Ensure we have a Whisper adapter available
try:
    whisper_adapter
except NameError:
    whisper_adapter = WhisperAdapter(model_name="base", language="en")

def _write_jsonl(path: str, rows: list[dict]):
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def _read_jsonl(path: str) -> list[dict]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                rows.append(json.loads(line))
    return rows

# 1) Compute durations quickly from ds audio arrays (no WAV writing needed for selection)
durations = []
for i, ex in enumerate(ds):
    a = ex["audio"]["array"]
    sr = ex["audio"]["sampling_rate"]
    dur_sec = float(len(a) / sr)
    durations.append((i, dur_sec))

durations.sort(key=lambda x: x[1], reverse=True)
selected = durations[:NUM_LONGEST_SAMPLES]
SELECTED_INDICES_LONGEST = [i for i, _ in selected]
selected_lengths = [d for _, d in selected]

print(f"Selected {len(SELECTED_INDICES_LONGEST)} longest samples.")
print("Duration stats (seconds):")
print(f"  total: {float(np.sum(selected_lengths)):.2f}")
print(f"  min:   {float(np.min(selected_lengths)):.2f}")
print(f"  max:   {float(np.max(selected_lengths)):.2f}")
print(f"  mean:  {float(np.mean(selected_lengths)):.2f}")

# 2) Build or reuse JSONL dataset
if REUSE_IF_EXISTS and Path(OUT_PATH).exists():
    fix_rows = _read_jsonl(OUT_PATH)
    print(f"Reused existing dataset: {OUT_PATH} ({len(fix_rows)} rows)")
else:
    fix_rows = []
    print(f"Generating JSONL at: {OUT_PATH}")

    for n, idx in enumerate(SELECTED_INDICES_LONGEST, start=1):
        ex = ds[int(idx)]
        gold_raw = ex["text"]

        wav_path = to_wav_16k_mono(ex, int(idx))
        whisper_raw, _ = whisper_adapter.transcribe(wav_path)

        row = {
            "id": n,
            "dataset_index": int(idx),
            "audio_sec": float(selected[n - 1][1]),
            "whisper_raw": whisper_raw,
            "gold_raw": gold_raw,
            "whisper_norm": normalise_text(whisper_raw),
            "gold_norm": normalise_text(gold_raw),
        }
        fix_rows.append(row)

        if n <= 5 or (n % 25 == 0) or (n == len(SELECTED_INDICES_LONGEST)):
            print(f"[{n}/{len(SELECTED_INDICES_LONGEST)}] idx={idx} aud_sec={row['audio_sec']:.2f}")
            print("  whisper_norm:", row["whisper_norm"][:140])
            print("  gold_norm:   ", row["gold_norm"][:140])

    _write_jsonl(OUT_PATH, fix_rows)
    print(f"Saved: {OUT_PATH} ({len(fix_rows)} rows)")

